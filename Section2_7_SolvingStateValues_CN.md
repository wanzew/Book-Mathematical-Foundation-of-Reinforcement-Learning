# 📘 2.7 通过 Bellman 方程求解状态值

计算给定策略的状态值（state values）是强化学习中的一个基本问题，通常称为**策略评估（policyevaluation）**。
本节介绍两种从 Bellman 方程计算状态值的方法：闭式解（closed-form solution）与迭代解（iterative solution）。

---

## 2.7.1 闭式解（Closed-form solution）

由于 $$v_π = r_π + γP_πv_π$$ 是一个线性方程组，它的闭式解可以表示为：

$$
v_π = (I - γP_π)^{-1}r_π
$$

一些关于 $(I - γP_π)^{-1}$ 的重要性质如下：

1.  **可逆性（Invertibility）**\
    矩阵 $(I - γP_π)$ 是可逆的。证明如下：根据 **Gershgorin
    圆盘定理**，矩阵 $I - γP_π$ 的每个特征值都位于至少一个 Gershgorin
    圆内。
    第 $i$ 个圆的圆心为：
    $$[I - γP_π]_{ii} = 1 - γp_π(s_i|s_i)$$
    半径为：
    $$sum_{j≠i} |[I - γP_π]_{ij}| = \sum_{j≠i} γp_π(s_j|s_i)$$
    因为 $γ < 1$，所以半径小于圆心的大小，即\
    $$sum_{j≠i} γp_π(s_j|s_i) < 1 - γp_π(s_i|s_i)$$
    因此所有 Gershgorin 圆都不包围原点，说明 $(I - γP_π)$
    没有零特征值，因此可逆。

2.  **非负性（Non-negativity）**\
    $(I - γP_π)^{-1} ≥ I$，意味着该矩阵的每个元素都是非负的，且不小于单位矩阵的对应元素。
    这是因为：

    $$
    (I - γP_π)^{-1} = I + γP_π + γ^2P_π^2 + … ≥ I
    $$

3.  **单调性（Monotonicity）**\
    对任意向量 $r ≥ 0$，有：
    $$(I - γP_π)^{-1}r ≥ r ≥ 0$$
    若 $r_1 ≥ r_2$，则：
    $$(I - γP_π)^{-1}r_1 ≥ (I - γP_π)^{-1}r_2$$

---

## 2.7.2 迭代解（Iterative solution）

虽然闭式解在理论上很有用，但在实际中计算代价较高，因为它涉及矩阵求逆操作。
因此，我们通常采用**迭代算法**来近似求解：

$$
v_{k+1} = r_π + γP_πv_k, \quad k = 0,1,2,… \tag{2.11}
$$

其中 $v_0$ 是任意初始猜测向量。
该算法生成一列状态值序列 $\{v_0,v_1,v_2,…\}$，并最终收敛至：

$$
v_k → v_π = (I - γP_π)^{-1}r_π, \quad 当\; k→∞. \tag{2.12}
$$

---

### 📦 Box 2.1：收敛性证明（Convergence proof of (2.12)）

定义误差项：

$$
δ_k = v_k - v_π
$$

只需证明 $δ_k → 0$。将 $v_{k+1} = r_π + γP_πv_k$ 代入：

$$
δ_{k+1} + v_π = r_π + γP_π(δ_k + v_π)
$$

化简得到：

$$
δ_{k+1} = γP_πδ_k
$$

递推展开：

$$
δ_{k+1} = γ^{k+1}P_π^{k+1}δ_0
$$

由于 $0 ≤ P_π ≤ 1$ 且 $γ < 1$，因此：

$$
γ^{k+1} → 0 \Rightarrow δ_{k+1} → 0
$$

即迭代算法收敛到唯一解 $v_π$。

---

## 2.7.3 🧩 示例说明（Illustrative examples）

接下来我们利用 (2.11) 的迭代算法求解几个状态值的示例。
示例如下图（Figure 2.7）所示：

- 橙色方格表示禁区（forbidden areas）
- 蓝色方格表示目标区域（target area）
- 奖励设置为：
  $$r_{boundary} = r_{forbidden} = -1, \quad r_{target} = +1$$
- 折扣因子：
  $$γ = 0.9$$

---

### 🟦 图 2.7(a)：两种"良好"策略及其状态值

两种策略在第四列的前两个状态有所不同，但它们的状态值完全相同。
说明不同策略可能产生相同的状态值。
表格右侧展示了每个格子的 $v(s)$ 数值，数值越大代表该状态越接近目标。

---

### 🟧 图 2.7(b)：两种"较差"策略及其状态值

这两种策略被认为是"差"的，因为它们在多个状态上的动作选择不合理。
从右侧表格可以看到，这些策略的状态值为负值且显著低于良好策略的结果。
说明策略的合理性可以通过状态值函数得到量化验证。

---

**图 2.7 总结：**\
良好策略对应的状态值高（趋近目标区域），\
而不良策略的状态值低（或为负）。
这体现了强化学习中"好策略=高价值"的核心思想。
