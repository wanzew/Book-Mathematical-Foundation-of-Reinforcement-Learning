
import numpy as np

gamma = 0.9
states = ["A", "B", "C", "D"]

# 奖励矩阵R
# R 是奖励矩阵，R[s, s'] 表示从状态s转移到状态s'获得的即时奖励：
# R[0, 0]: A->A 的奖励（=10） R[0, 1]: A->B 的奖励（=2） R[0, 2]: A->C 的奖励（=0） R[0, 3]: A->D 的奖励（=0）
# R[1, 0]: B->A 的奖励（=0）  R[1, 1]: B->B 的奖励（=0） R[1, 2]: B->C 的奖励（=3） R[1, 3]: B->D 的奖励（=0）
# R[2, 0]: C->A 的奖励（=0）  R[2, 1]: C->B 的奖励（=0） R[2, 2]: C->C 的奖励（=0） R[2, 3]: C->D 的奖励（=0）
# R[3, 0]: D->A 的奖励（=0）  R[3, 1]: D->B 的奖励（=0） R[3, 2]: D->C 的奖励（=0） R[3, 3]: D->D 的奖励（=0）
R = np.array([[10, 2, 0, 0],
              [0,  0, 3, 0],
              [0,  0, 0, 0],
              [0,  0, 0, 0]])

# 转移概率矩阵P
# P 是状态转移概率矩阵，P[s, s'] 表示从状态s转移到状态s'的概率：
# P[0, 0]: A->A 的概率（=0.2） P[0, 1]: A->B 的概率（=0.8） P[0, 2]: A->C 的概率（=0） P[0, 3]: A->D 的概率（=0）
# P[1, 0]: B->A 的概率（=0）   P[1, 1]: B->B 的概率（=0）   P[1, 2]: B->C 的概率（=1） P[1, 3]: B->D 的概率（=0）
# P[2, 0]: C->A 的概率（=0）   P[2, 1]: C->B 的概率（=0）   P[2, 2]: C->C 的概率（=0） P[2, 3]: C->D 的概率（=1）
# P[3, 0]: D->A 的概率（=0）   P[3, 1]: D->B 的概率（=0）   P[3, 2]: D->C 的概率（=0） P[3, 3]: D->D 的概率（=1）
P = np.array([[0.2, 0.8, 0, 0],
              [0,   0,   1, 0],
              [0,   0,   0, 1],
              [0,   0,   0, 1]])

v = np.zeros(4)
for _ in range(50):
    # 每一项 sum_a P[s,a] * [R[s,a] + gamma*v[a]]
    v_new = np.zeros_like(v)
    for s in range(4):
        v_new[s] = np.sum(P[s] * (R[s] + gamma * v))
    v = v_new

print("v_pi =", np.round(v, 3))