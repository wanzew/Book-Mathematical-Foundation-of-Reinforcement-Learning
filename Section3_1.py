import numpy as np

# 状态列表
states = ["A", "B", "C"]
# 折扣因子
gamma = 0.9

# 这是一个马尔可夫决策过程（MDP）中的策略迭代问题。
# 状态集合为A、B、C，每个状态有三个动作可选。
# 给定转移概率矩阵P和奖励矩阵R，以及折扣因子gamma，
# 目标是通过策略迭代（包括策略评估和策略提升两步）求解最优状态值函数和最优策略。


# 奖励矩阵R
# R 是奖励矩阵，R[s, a] 表示在状态s下执行动作a获得的即时奖励：
# R[0, 0]: 状态A执行动作0的奖励（=0） R[0, 1]: 状态A执行动作1的奖励（=1） R[0, 2]: 状态A执行动作2的奖励（=0）
# R[1, 0]: 状态B执行动作0的奖励（=0） R[1, 1]: 状态B执行动作1的奖励（=0） R[1, 2]: 状态B执行动作2的奖励（=2）
# R[2, 0]: 状态C执行动作0的奖励（=0） R[2, 1]: 状态C执行动作1的奖励（=0） R[2, 2]: 状态C执行动作2的奖励（=0）
R = np.array([
 #   a0 a1 a2
    [0, 1, 0], # A (State)
    [0, 0, 2], # B (State)
    [0, 0, 0]  # C (State)
])

# 转移概率矩阵P，P[s,a,s']表示从状态s执行动作a转移到s'的概率
P = np.array([
    [[0, 1, 0],  # 状态A，a0只会到B
     [0, 0, 1],  # 状态A，a1只会到C
     [0, 0, 1]], # 状态A，a2只会到C

    [[0, 1, 0],  # 状态B
     [0, 0, 1],
     [0, 0, 1]],

    [[0, 1, 0],  # 状态C
     [0, 0, 1],
     [0, 0, 1]]
])

# 初始化状态值函数v（长度为状态数）
v = np.zeros(3)
# 初始化策略，每个状态选择的动作（用动作索引，初始为0）
policy = np.zeros(3)

for iteration in range(10):
    # 策略评估步骤：多次迭代以收敛于当前策略下的状态值
    # 这段代码是在做“策略评估（policy evaluation）”步骤，用固定策略下估算每个状态的价值v
    # 但这里写的是：np.sum(R, axis=1) + gamma * P[0].dot(v)
    # 实际上 np.sum(R, axis=1) 是对每个状态的所有动作奖励求和，
    # P[0].dot(v) 是只取了状态A（即第一个状态）的转移概率，这不是通用的策略评估算法！
    # 一般的策略评估应该是：对每个状态s，v[s] = R[s, policy[s]] + gamma * np.dot(P[s, policy[s]], v)
    # policy[s] 表示当前策略在s状态选的动作（a），P[s, a, :] 是动作a下转移到各 s' 的概率
    for _ in range(5):
        v = np.sum(R, axis=1) + gamma * P[0].dot(v)
    
    # 策略改进步骤：通过贪心方式提升当前策略
    q = np.zeros((3, 3))  # 每个状态-动作对的价值
    for s in range(3):    # 遍历所有状态
        # 向量化实现：直接一次性计算所有动作的q值
        q[s] = R[s] + gamma * P[s].dot(v)
        # 选取具有最大q值的动作更新策略
        policy[s] = np.argmax(q[s])

# 输出最终策略和状态值
print("改进后的策略:", policy)
print("状态值:", v)