import numpy as np

# 状态列表
states = ["A", "B", "C"]
# 折扣因子
gamma = 0.9

# 这是一个马尔可夫决策过程（MDP）中的策略迭代问题。
# 状态集合为A、B、C，每个状态有三个动作可选。
# 给定转移概率矩阵P和奖励矩阵R，以及折扣因子gamma，
# 目标是通过策略迭代（包括策略评估和策略提升两步）求解最优状态值函数和最优策略。


# 奖励矩阵R
# R 是奖励矩阵，R[s, a] 表示在状态s下执行动作a获得的即时奖励：
# R[0, 0]: 状态A执行动作0的奖励（=0） R[0, 1]: 状态A执行动作1的奖励（=1） R[0, 2]: 状态A执行动作2的奖励（=0）
# R[1, 0]: 状态B执行动作0的奖励（=0） R[1, 1]: 状态B执行动作1的奖励（=0） R[1, 2]: 状态B执行动作2的奖励（=2）
# R[2, 0]: 状态C执行动作0的奖励（=0） R[2, 1]: 状态C执行动作1的奖励（=0） R[2, 2]: 状态C执行动作2的奖励（=0）
R = np.array([
 #   a0 a1 a2
    [0, 1, 0], # A (State)
    [0, 0, 2], # B (State)
    [0, 0, 0]  # C (State)
])

# 转移概率矩阵P，P[s,a,s']表示从状态s执行动作a转移到s'的概率
P = np.array([
    [[0, 1, 0],  # 状态A，a0只会到B
     [0, 0, 1],  # 状态A，a1只会到C
     [0, 0, 1]], # 状态A，a2只会到C

    [[0, 1, 0],  # 状态B
     [0, 0, 1],
     [0, 0, 1]],

    [[0, 1, 0],  # 状态C
     [0, 0, 1],
     [0, 0, 1]]
])

# 初始化状态值函数v（长度为状态数）
v = np.zeros(3)
# 初始化策略，每个状态选择的动作（用动作索引，初始为0）
policy = np.zeros(3)

for iteration in range(10):
    # 策略评估步骤：多次迭代以收敛于当前策略下的状态值
    for _ in range(5):
        v = np.sum(R, axis=1) + gamma * P[0].dot(v)
    
    # 策略改进步骤：通过贪心方式提升当前策略
    q = np.zeros((3, 3))  # 每个状态-动作对的价值
    for s in range(3):    # 遍历所有状态
        for a in range(3):    # 遍历所有动作
            # 计算采取动作a的预期回报
            q[s, a] = R[s, a] + gamma * P[s, a].dot(v)
        # 选取具有最大q值的动作更新策略
        policy[s] = np.argmax(q[s])

# 输出最终策略和状态值
print("改进后的策略:", policy)
print("状态值:", v)