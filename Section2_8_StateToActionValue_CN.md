# 📘 2.8 从状态值到动作值（State → Action Value）

> 本节把 **状态值函数** $v_\pi(s)$ 推广为 **动作值函数** > $q_\pi(s,a)$，并给出两者的严格关系与推导，说明如何由 $v_\pi$ 计算
> $q_\pi$，以及如何利用 $q_\pi$ 提取最优（或改进）策略。

---

## 1. 动作值函数的定义

**动作值（Action Value）** 定义为：在状态 $s$ 执行动作 $a$
后、并继续按照策略 $\pi$ 行为时的**期望累计折扣回报**：

$$
q_\pi(s,a) \;=\; \mathbb{E}\!\left[\,G_t \,\big|\, S_t=s,\; A_t=a\,\right].
$$

直观理解：$q_\pi(s,a)$ 衡量"在 $s$ 里先做 $a$，然后继续按 $\pi$
走"的长期价值。

---

## 2. $v_\pi$ 与 $q_\pi$ 的关系

### 2.1 由 $q_\pi$ 得到 $v_\pi$（加权平均式）

在策略 $\pi$ 下，状态值是对所有动作值按策略分布加权求和：
$$ v*\pi(s) \;=\; \sum*{a} \pi(a|s)\, q\_\pi(s,a). $$

### 2.2 由 $v_\pi$ 计算 $q_\pi$（一次展开式）

根据一步展开与全期望公式，

$$
q*\pi(s,a) \;=\; \sum*{r} p(r\,|\,s,a)\, r \;+\; \gamma \sum*{s'} p(s'\,|\,s,a)\, v*\pi(s').
$$

若将 $v_\pi$ 再按上式替换为 $q_\pi$ 的加权平均，可得到 $q_\pi$ 的闭式
Bellman 形式（见 §3）。

---

## 3. 动作值的 Bellman 方程（$q$-Bellman）

**$q_\pi$ 的 Bellman 方程：**

$$
q*\pi(s,a) \;=\; \sum*{r} p(r\,|\,s,a)\, r \;+\; \gamma \sum*{s'} p(s'\,|\,s,a)\, \sum*{a'} \pi(a'|s')\, q\_\pi(s',a').
$$

> 含义：先拿到即时奖励的期望，然后看"下一状态 $s'$
> 下的动作值期望"，并对其折扣求和。

当 $\pi$ 为最优策略 $\pi^*$ 时，上式中的"对 $\pi$
加权平均"可替换成"取最大"，得到最优形式（见 §5）。

---

## 4. 从 $v_\pi$ 与 $P,\; r$ 直接构造 $q_\pi$（工程计算）

在工程实现中，常见两种路径：

### 4.1 **路径 A：先评估 $v_\pi$，再由 $v_\pi \to q_\pi$**

1.  用闭式或迭代法计算

    $$
      v*\pi = r*\pi + \gamma P*\pi v*\pi \quad \Rightarrow \quad v*\pi = (I-\gamma P*\pi)^{-1} r\_\pi.
    $$

2.  逐状态动作计算 $$
    q_\pi(s,a) = \sum_{r} p(r|s,a)\,r + \gamma \sum_{s'} p(s'|s,a)\, v_\pi(s').
    $$

**优点**：只要有 $v_\pi$，即可一次性得到所有 $(s,a)$ 的 $q_\pi$。

### 4.2 **路径 B：直接评估 $q_\pi$（不经 $v_\pi$）**

对所有 $(s,a)$ 直接做迭代： $$
q^{(k+1)}(s,a) \;=\; \sum_{r} p(r|s,a)\, r \;+\; \gamma \sum_{s'} p(s'|s,a) \sum_{a'} \pi(a'|s')\, q^{(k)}(s',a').
$$ 收敛后得 $q_\pi$，再由 $$
v_\pi(s) = \sum_a \pi(a|s)\,q_\pi(s,a)
$$ 恢复 $v_\pi$。

**优点**：在控制问题中，$q_\pi$
直接对应"选哪个动作"，更贴近策略改进/贪心选择。

---

## 5. 最优情形与策略改进

当考虑**最优策略**时，动作值满足：

$$
q^*(s,a) \;=\; \sum*{r} p(r|s,a)\, r \;+\; \gamma \sum*{s'} p(s'|s,a)\, \max\_{a'} q^*(s',a').
$$

由此可得： - **最优状态值**

$$
v^*(s) \;=\; \max\_{a} q^*(s,a).

$$

- **最优贪心策略**
  $$
      \pi^*(a|s) \;=\;
      \begin{cases}
      1, & a \in \arg\max_{a'} q^*(s,a'), \\
      0, & \text{otherwise.}
      \end{cases}
  $$

这正是值迭代/策略迭代、Q-learning 与 Actor--Critic 中"基于 $q$
的贪心/近贪心选动作"的理论依据。

---

## 6. 优势函数（Advantage）与基线

定义**优势函数**：

$$
A*\pi(s,a) \;=\; q*\pi(s,a) - v\_\pi(s).
$$

- 若 $A_\pi(s,a)>0$，说明动作 $a$ 比当前策略在 $s$ 的平均动作更好；
- 若 $A_\pi(s,a)<0$，则更差。

优势函数能稳定策略梯度方法（作为"基线"减法），并用于解释"策略改进为何有效"。

---

## 7. 数值与实现要点（工程实践）

1.  **由 $v_\pi \to q_\pi$ 更稳健**：先评估 $v_\pi$（线性系统或 DP迭代），再按"一步展开式"计算 $q_\pi$，数值上更稳且便于并行。\
2.  **直接 $q$-迭代**：适合与控制/改进策略无缝衔接（如 Q-learning 的off-policy 更新）。\
3.  **近似表示**：在大状态-动作空间中，$q_\pi$ 更适合用函数逼近（如NN）直接拟合。\
4.  **探索-利用权衡**：基于 $q$ 的 $\varepsilon$-greedy/softmax 更直观（直接对动作打分）。

---

## 8. 小结与自测

**小结**\

- $v_\pi$ 与 $q_\pi$ 通过"策略加权平均"和"一步展开"双向联系；\
- $q_\pi$ 的 Bellman 方程把"即时奖励 + 折扣未来价值"统一起来；\
- 在求最优时，把加权平均换成 $\max$ 即得到 $q^*$ 与 $v^*$；\
- 控制/选动作场景下，$q$ 比 $v$ 更直接。

**自测**\

1. 证明 $v_\pi(s)=\sum_a \pi(a|s)q_\pi(s,a)$；\
2. 给定 $v_\pi$，推导 $q_\pi$ 的一次展开式；\
3. 写出 $q^*(s,a)$ 的最优 Bellman 方程，并解释为何出现 $\max$；\
4. 在何种场景下优先选择"先 $v$ 后 $q$"的路径？何时直接做
   $q$-迭代更合适？
   $$
