# ------ 第 2.2 节 · Bootstrapping 与回报递推 ------

> 本节介绍强化学习中的核心递推思想------**Bootstrapping（自举）**。\
> 它让智能体在不必等到整个回合结束的情况下，就能用已有估计更新自己对未来的判断。\
> 这正是动态规划（Dynamic Programming）与时序差分（TD）学习的数学基础。

------------------------------------------------------------------------

## 🎯 学习目标

学习完本节后，你应能：\
1. 理解回报 $G_t$ 的递推定义；\
2. 掌握 Bootstrapping 的基本逻辑；\
3. 能用简单数值例题手算出 $v(S_t)$ 的更新；\
4. 明白 Bootstrapping 与蒙特卡洛（Monte Carlo）方法的区别。

------------------------------------------------------------------------

## 📘 一、回报的定义与展开

在强化学习中，我们衡量一个状态的好坏，主要通过"**未来的累计折扣奖励**"：

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \tag{2.1}
$$

这称为 **折扣回报（discounted return）**。\
若回合在 T 时刻终止，则：

$$
G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}.
$$

------------------------------------------------------------------------

## 📐 二、回报的递推关系

我们注意到，式 (2.1) 可以拆解为：

$$
\begin{aligned}
G_t &= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \\
    &= R_{t+1} + \gamma G_{t+1}. \tag{2.2}
\end{aligned}
$$

这就是 **回报的递推定义（Recursive Definition of Return）**。\
它说明：\
\> 当前的回报 $G_t$ = 即时奖励 + 折扣后的未来回报。

------------------------------------------------------------------------

## 💡 三、Bootstrapping 思想

在实际学习过程中，我们往往**无法提前知道 $G_{t+1}$ 的真实值**，\
因此用当前的估计值 $v(S_{t+1})$ 来代替：

$$
v(S_t) \leftarrow R_{t+1} + \gamma v(S_{t+1}). \tag{2.3}
$$

这就是 **Bootstrapping（自举）**：\
\> 用自己的预测（$v(S_{t+1})$）更新自己（$v(S_t)$）。

直观理解：智能体"踩着自己的肩膀向上爬"。\
通过反复递推，智能体在整个状态空间中传播关于未来的知识。

------------------------------------------------------------------------

## 🧩 四、直觉图景

想象一个五格世界：

    S0 → S1 → S2 → S3 → Goal

-   每次移动奖励 $R=-1$；
-   终点奖励 $R=0$；
-   折扣因子 $\gamma=0.9$。

我们从终点开始反推：

$$
v(S_4)=0, \quad v(S_3)=-1+0.9(0)=-1, \\
v(S_2)=-1+0.9(-1)=-1.9, \quad v(S_1)=-2.71.
$$

→ 每个状态的价值是"**离目标越远，期望损失越大**"。\
这就是 Bootstrapping 递推更新的结果。

------------------------------------------------------------------------

## 🔢 五、数值例题

假设：\
- $R_{t+1}=0.8$， - $\gamma=0.9$， - 当前估计 $v(S_{t+1})=1.0$。

根据 (2.3)：

$$
v(S_t) = R_{t+1} + \gamma v(S_{t+1}) = 0.8 + 0.9(1.0) = 1.7.
$$

下一次更新后，若 $v(S_{t+1})$ 改进为 1.2，则：

$$
v(S_t)=0.8+0.9(1.2)=1.88.
$$

智能体就这样**边走边修正自己的预测**。

------------------------------------------------------------------------

## 🧮 六、Python伪代码：Bootstrapping 更新循环

``` python
gamma = 0.9
alpha = 0.1  # 学习率

# 初始状态值估计
v = {"S0": 0.0, "S1": 0.0, "S2": 0.0, "S3": 0.0, "Goal": 0.0}
rewards = {"S0": -1, "S1": -1, "S2": -1, "S3": 0}

# Bootstrapping 更新
for episode in range(10):
    for s in ["S0", "S1", "S2", "S3"]:
        next_s = {"S0": "S1", "S1": "S2", "S2": "S3", "S3": "Goal"}[s]
        v[s] += alpha * (rewards[s] + gamma * v[next_s] - v[s])

print(v)
```

这段代码展示了时序差分（TD(0)）的基本形式，\
每步都利用"预测的预测"来进行迭代更新。

------------------------------------------------------------------------

## 🧠 七、自测题

1.  证明式 (2.2) 可以展开回式 (2.1)。\
2.  当 $\gamma=0$ 时，Bootstrapping 更新等价于什么？\
3.  为什么 Bootstrapping 在环境长时间任务（如持续控制）中特别重要？\
4.  与蒙特卡洛方法相比，Bootstrapping 有何优势与风险？

------------------------------------------------------------------------

## 📘 八、与第 2.3 节的衔接

本节展示了如何通过递推与自举思想计算回报。\
接下来的 §2.3 将引入**状态值函数 $v_\pi(s)$ 的形式定义**， 把
Bootstrapping 的经验公式转化为数学期望，\
为后续的 Bellman 方程奠定理论基础。
