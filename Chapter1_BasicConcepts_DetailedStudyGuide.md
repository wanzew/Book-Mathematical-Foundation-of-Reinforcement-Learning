# 📘 Chapter 1: Basic Concepts --- 深度阅读学习笔记

> 🔍 本章是强化学习（Reinforcement Learning,
> RL）的起点，旨在建立一套系统的思维框架。\
> 你将从网格世界出发，理解状态、动作、策略、奖励、回报和马尔可夫决策过程（MDP）等基础概念。\
> 后续所有算法（如 Bellman 方程、Value Iteration、TD、Actor-Critic
> 等）都基于这些概念。

------------------------------------------------------------------------

## 🎯 学习目标

学习完本章后，你应能回答：

1.  强化学习与监督学习、无监督学习有何根本区别？\
2.  什么是状态（State）、动作（Action）、奖励（Reward）、策略（Policy）？\
3.  如何用马尔可夫决策过程（MDP）形式化描述一个强化学习问题？\
4.  什么是"回报（Return）"与"折扣因子（Discount Factor）"？\
5.  MDP 与马尔可夫链（Markov Chain）的关系是什么？

------------------------------------------------------------------------

## 1.1 🌍 Grid World 引例

### 🧩 场景简介

假设一个机器人在 3×3 网格世界中移动： - 白色格子为可行区域；\
- 橙色格子为**禁止区域**（forbidden cells）；\
- 红色格子为**目标区域**（target cell）。

机器人每次只能移动一步：上、下、左、右或保持不动。

其任务：\
**从任意初始格出发，找到一条最优策略（policy），到达目标且避开禁区与边界。**

------------------------------------------------------------------------

### 💡 思考

> 为什么称之为"强化"学习？\
> 👉
> 因为智能体（agent）通过"与环境的交互"获得反馈信号（reward），并根据奖励的累计值调整行为。

------------------------------------------------------------------------

## 1.2 🧠 状态与动作（State & Action）

### 🌱 直观理解

-   **状态（State）**：描述智能体当前所处的环境信息（如机器人所在格子）。\
-   **动作（Action）**：智能体在该状态下可采取的行为（如上、下、左、右、停）。

### 📐 数学定义

-   状态集合：\
    \[ S = { s_1, s_2, ..., s_9 } \]

-   动作集合：\
    \[ A = { a_1, a_2, a_3, a_4, a_5 } \] 其中
    a₁=上，a₂=右，a₃=下，a₄=左，a₅=停。

-   每个状态的动作空间可不同，如：\
    \[ A(s_1) = { a_2, a_3, a_5 } \] （因为向上/左会越界）

------------------------------------------------------------------------

### 🧩 自测问题

1.  如果一个状态没有任何可执行动作，这个状态是什么类型？\
2.  为什么我们要区分全局动作集合 A 与状态依赖动作集合 A(s)？

------------------------------------------------------------------------

## 1.3 🔄 状态转移（State Transition）

### 🌱 概念

执行某个动作 a 后，智能体从状态 s 转移到下一个状态 s′：\
\[ s `\xrightarrow{a}`{=tex} s' \]

转移可以是： - **确定性（Deterministic）**：结果唯一； -
**随机性（Stochastic）**：结果有概率分布。

### 📐 数学表示

\[ p(s'\|s,a) = `\Pr`{=tex}(S\_{t+1}=s' \| S_t=s, A_t=a) \]

满足：\
\[ `\sum`{=tex}\_{s'} p(s'\|s,a) = 1 \]

### 💡 示例

-   若在 s₁ 执行右移：p(s₂\|s₁,a₂)=1\
-   若环境有风干扰：可能 p(s₅\|s₁,a₂)=0.2

------------------------------------------------------------------------

### 🧠 思考题

1.  如果环境完全确定，p(s′\|s,a) 具有什么性质？\
2.  若随机性来自外界扰动（如风），应如何建模？

------------------------------------------------------------------------

## 1.4 🧭 策略（Policy）

### 🌱 定义

策略 π 告诉智能体在每个状态下如何选择动作。\
可分为： - **确定性策略（Deterministic Policy）**\
\[ π(a\|s) ∈ {0,1} \] - **随机性策略（Stochastic Policy）**\
\[ π(a\|s) ∈ \[0,1\], `\quad `{=tex}`\sum`{=tex}\_a π(a\|s)=1 \]

### 💡 直观解释

-   决定性策略：固定选择同一动作（如 s₁→右移）。\
-   随机策略：在多种动作中按概率抽样（如右移 0.5，下移 0.5）。

### 📋 表格表示

  状态   上    右    下    左    停
  ------ ----- ----- ----- ----- -----
  s₁     0     0.5   0.5   0     0
  s₂     0     0     1     0     0
  ...    ...   ...   ...   ...   ...

------------------------------------------------------------------------

### 🧠 自测

1.  为什么随机策略在早期探索阶段更优？\
2.  策略能否依赖于历史（非马尔可夫性）？会导致什么问题？

------------------------------------------------------------------------

## 1.5 🎯 奖励（Reward）

### 🌱 定义

执行动作后环境反馈的信号：\
\[ r = r(s,a) \]

### 📋 Grid World 设计示例

  情况       奖励值
  ---------- --------
  触碰边界   -1
  进入禁区   -1
  达到目标   +1
  其他移动   0

奖励的作用是**引导学习方向**。

### 💡 关键洞察

-   奖励设计即目标建模。\
-   奖励太稀疏 → 学习困难；\
-   奖励太密集 → 易陷入局部最优。

------------------------------------------------------------------------

### 📐 奖励的概率模型

奖励可随机：\
\[ p(r\|s,a) \]

例如：\
\[ p(r=-1\|s_1,a_1)=1 \]\
表示必定得到 -1 奖励。

------------------------------------------------------------------------

### 🧠 思考

1.  为什么"相对奖励"而非"绝对奖励"决定策略？\
2.  若所有奖励都乘以常数 c\>0，最优策略是否改变？

------------------------------------------------------------------------

## 1.6 ⛓️ 轨迹、回报与情节（Trajectory, Return, Episode）

### 🧩 定义

-   **轨迹**：状态--动作--奖励序列\
    \[ s_0,a_0,r_1,s_1,a_1,r_2,... \]

-   **回报（Return）**：累计奖励\
    \[ G_t = r\_{t+1} + r\_{t+2} + ... + r_T \]

### ⚖️ 折扣回报（Discounted Return）

\[ G_t = `\sum`{=tex}*{k=0}^`\infty `{=tex}`\gamma`{=tex}^k r*{t+k+1},
`\quad `{=tex}`\gamma`{=tex}∈(0,1) \]

推导：\
\[ G_t = r\_{t+1} + `\gamma `{=tex}r\_{t+2} + `\gamma`{=tex}\^2
r\_{t+3} + ... \] 若 r=1, 则\
\[ G = `\frac{1}{1-\gamma}`{=tex} \]

γ 的作用：平衡**短期收益**与**长期收益**。

------------------------------------------------------------------------

### 🧩 图示说明（文字描述）

    s1 --a2,r=0--> s2 --a3,r=0--> s5 --a3,r=0--> s8 --a2,r=+1--> s9
    累积回报 G = 0 + 0 + 0 + 1 = 1

------------------------------------------------------------------------

### 🧠 思考

1.  若 γ=0 与 γ=1 分别意味着什么？\
2.  为什么需要折扣因子？

------------------------------------------------------------------------

## 1.7 🧩 马尔可夫决策过程（Markov Decision Process, MDP）

### 📐 定义

\[ MDP = `\langle `{=tex}S, A, R, p(s'\|s,a), p(r\|s,a), π(a\|s)
`\rangle `{=tex}\]

组成：

  元素   含义
  ------ ----------
  S      状态空间
  A      动作集合
  R      奖励集合
  p(s′   s,a)
  p(r    s,a)
  π(a    s\)

### ⚙️ 马尔可夫性

\[ p(s\_{t+1}\|s_t,a_t,s\_{t-1},...) = p(s\_{t+1}\|s_t,a_t) \]

未来只依赖当前状态与动作 → **无记忆性**。

------------------------------------------------------------------------

### 🧩 Agent--Environment 交互闭环（文字图）

    State s_t
       ↓
    Policy π(a|s_t)
       ↓
    Action a_t
       ↓
    Environment
       ↓
    Reward r_t, New state s_{t+1}
       ↓
    Agent updates π based on feedback

------------------------------------------------------------------------

## 🧩 1.8 章节总结与思维导图

强化学习核心闭环：

    S (state)
    ↓
    π(a|s) — policy —→ A (action)
    ↓
    Environment dynamics p(s′|s,a), p(r|s,a)
    ↓
    Reward r, New state s′
    ↓
    → 更新策略 π

------------------------------------------------------------------------

## 🧮 1.9 章节练习与思考

### ✅ 基础题

1.  定义状态空间 S 与动作空间 A，并说明它们的区别。\
2.  什么是折扣回报？给出 γ=0.9 时 r=1 的无限回报。\
3.  为什么奖励的仿射变换不改变最优策略？

### 💭 进阶题

4.  若环境中风向随机，使得右移 70%、下移 30%，如何写出 p(s′\|s,a)？\
5.  在 Grid World 中，若想让智能体"更快到达目标"，奖励设计应如何修改？

### 💡 提示区

-   题 2：G = 1/(1−γ)\
-   题 3：因为奖励函数乘常数或加常数不会改变 argmax。

------------------------------------------------------------------------

## 📚 下一步学习建议

👉 阅读 Chapter 2: **Bellman Equation**\
学习如何基于 MDP 定义构造值函数（Value Function）并推导最优性方程。
