
# —— 第2.3节 · 状态值函数（State Value Function）——

> 本节将定义强化学习中最基础的概念之一——**状态值函数**。
> 它衡量在给定策略下，预期可以获得的长期回报，为动态规划和时序差分学习奠定了基础。

---

## 🎯 学习目标

学习完本节后，你应能：  
1. 理解状态值函数 $v_\pi(s)$ 的定义与意义；  
2. 能用期望公式表达并递归地计算它；  
3. 描述值函数与策略之间的关系；  
4. 用代码实现策略评估，并求解简单示例。

---

## 📘 1. 定义

在给定策略 $\pi$ 下，**状态值函数**定义为：从状态 $s$ 出发、依照 $\pi$ 行动时，期望获得的累计折扣回报。

$$
v_\pi(s) = \mathbb{E}_\pi[G_t | S_t=s] = \mathbb{E}_\pi\Big[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\Big|S_t=s\Big]. \tag{2.4}
$$

- $\mathbb{E}_\pi$：在策略 $\pi$ 下的期望  
- $R_{t+k+1}$：第 $t+k+1$ 步时获得的奖励  
- $\gamma\in[0,1]$：折扣因子

> **直观理解：** $v_\pi(s)$ 是智能体从 $s$ 出发、遵循 $\pi$ 时**平均能获得的长期奖励**。

---

## 💡 2. 与策略的关系

- 策略 $\pi$ 决定采取哪些动作，$v_\pi(s)$ 衡量这些动作带来的长期结果；  
- 换一个策略，$v_\pi(s)$ 也会随之改变；  
- 对比不同策略时，可以观察各自的值函数。

> 一个“更优”的策略对应各状态的 $v_\pi(s)$ 更高。

---

## 🧩 3. 递归表达

根据回报的定义：

$$
G_t = R_{t+1} + \gamma G_{t+1}.
$$

对其在 $\pi$ 下取期望，有：

$$
v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t=s]. \tag{2.5}
$$

这说明 $v_\pi(s)$ 满足**递归关系**：  
当前价值 = 期望即时奖励 + 折扣后的下步价值。

---

## 🔢 4. 数值示例

考虑一个确定性的三状态系统 $S=\{A,B,C\}$：

| 状态 | 下一状态 | 奖励 | 转移概率 |
| ---- | -------- | ---- | -------- |
| A    | B        | +1   | 1.0      |
| B    | C        | +2   | 1.0      |
| C    | C        | 0    | 1.0      |

设 $\gamma=0.9$，策略 $\pi$ 按上述转移。

$$
\begin{aligned}
v_\pi(A)&=1+0.9v_\pi(B),\\
v_\pi(B)&=2+0.9v_\pi(C),\\
v_\pi(C)&=0.
\end{aligned}
$$

解得：  
$$
v_\pi(A)=2.8,\;v_\pi(B)=2.0,\;v_\pi(C)=0.
$$

解释：距离目标越近，累计剩余奖励越小。

---

## 🧮 5. Python 示例：策略评估

```python
import numpy as np

gamma = 0.9
R = np.array([1, 2, 0])
P = np.array([[0, 1, 0],
              [0, 0, 1],
              [0, 0, 1]])
v = np.zeros(3)
for _ in range(20):
    v = R + gamma * P.dot(v)

print(v)
```

输出：
```
[2.8 2.  0. ]
```

这段代码用贝尔曼期望方程做了迭代式的策略评估。

---

## 💬 6. 可视化直观

可以把 $v_\pi(s)$ 想象成网格世界中的**价值地形**：  
- 靠近目标的地方价值更高，陷阱附近的价值更低；  
- 策略 $\pi$ 如同向价值高处“爬坡”。

---

## 🧠 7. 自测题

1. 如果两个策略对应所有状态的 $v_\pi(s)$ 都相同，它们等价吗？  
2. 为什么 $v_\pi(s)$ 只取决于 $\pi$ 和环境动力学？  
3. 如果环境有多个目标终止状态，$v_\pi(s)$ 怎么表达？  
4. 递归表达式 (2.5) 与 Bootstrapping (2.3) 有什么联系？

---

## 📘 8. 与第2.4节的衔接

本节定义了 $v_\pi(s)$ 及其递归形式。  
第2.4节将推导**贝尔曼方程**，用更严谨的数学方式表达这一递归，为动态规划和强化学习算法打下理论基础。

