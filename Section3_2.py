import numpy as np

"""
æœ¬ç¤ºä¾‹æ„å»ºä¸€ä¸ªç®€å• MDPï¼ˆå’Œä½  3.1 èŠ‚ä¸­çš„ç¤ºä¾‹é£æ ¼ç±»ä¼¼ï¼‰

çŠ¶æ€:   A=0, B=1, C=2
åŠ¨ä½œ:   0,1,2ï¼ˆä¸ºäº†ç»Ÿä¸€çŸ©é˜µç»´åº¦ï¼Œæˆ‘ä»¬ä»¤ä¸‰ç§åŠ¨ä½œï¼Œä½†ä¸æ˜¯æ‰€æœ‰éƒ½æœ‰æ•ˆï¼‰

å¥–åŠ±:
    A â†’ B å¥–åŠ± 1
    B â†’ C å¥–åŠ± 2
    C â†’ C å¥–åŠ± 0
"""

# çŠ¶æ€æ•°é‡ nS: State æ•°é‡ï¼ˆA, B, Cï¼‰
nS = 3
# åŠ¨ä½œæ•°é‡ nA: Action æ•°é‡ï¼ˆ0, 1, 2ï¼Œä¸ºäº†ç»Ÿä¸€çŸ©é˜µï¼Œè™½ç„¶æœ‰çš„çŠ¶æ€ä¸èƒ½ç”¨æ‰€æœ‰åŠ¨ä½œï¼Œ
# ä½†ä¸ºäº†æ–¹ä¾¿è®¡ç®—ï¼Œæˆ‘ä»¬å‡è®¾æ‰€æœ‰çŠ¶æ€éƒ½èƒ½ç”¨æ‰€æœ‰åŠ¨ä½œï¼‰
nA = 3
gamma = 0.9

# å¥–åŠ±çŸ©é˜µ R[s,a]
R = np.array([
    [0, 1, 0],   # A: action 1 -> reward = 1
    [0, 0, 2],   # B: action 2 -> reward = 2
    [0, 0, 0]    # C: loop with zero reward
], dtype=float)

# è½¬ç§»çŸ©é˜µ P[s,a,s']
# P æ˜¯è½¬ç§»æ¦‚ç‡å¼ é‡ï¼Œç»´åº¦åˆ†åˆ«ä¸º (çŠ¶æ€æ•°é‡ nS, åŠ¨ä½œæ•°é‡ nA, ä¸‹ä¸€ä¸ªçŠ¶æ€æ•°é‡ nS)
# å³ P[s, a, s'] è¡¨ç¤ºï¼šåœ¨çŠ¶æ€ s ä¸‹é‡‡å–åŠ¨ä½œ aï¼Œè½¬ç§»åˆ°çŠ¶æ€ s' çš„æ¦‚ç‡
P = np.zeros((nS, nA, nS))

# A -> B
P[0,1,1] = 1 # çŠ¶æ€Aæ—¶é‡‡å– a1 åŠ¨ä½œè½¬ç§»åˆ°çŠ¶æ€Bçš„æ¦‚ç‡ä¸º1
# B -> C
P[1,2,2] = 1 # çŠ¶æ€Bæ—¶é‡‡å– a2 åŠ¨ä½œè½¬ç§»åˆ°çŠ¶æ€Cçš„æ¦‚ç‡ä¸º1
# C -> C
P[2,:,2] = 1 # çŠ¶æ€Cæ—¶é‡‡å–ä»»æ„åŠ¨ä½œè½¬ç§»åˆ°çŠ¶æ€Cçš„æ¦‚ç‡ä¸º1

# ----------- å·¥å…·å‡½æ•°ï¼šç­–ç•¥è¯„ä¼° v_pi(s) (Bellman Expectation Eq.) -----------
def policy_evaluation(policy, tol=1e-6, gamma=0.9):
    """
    è§£çº¿æ€§æ–¹ç¨‹ç»„ v = R_pi + gamma * P_pi * v
    policy: ä¸€ä¸ªé•¿åº¦ä¸º nS çš„æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å½“å‰ç­–ç•¥é€‰æ‹©çš„åŠ¨ä½œ
    """
    v = np.zeros(nS) # åˆå§‹çŠ¶æ€å€¼ä¸º0
    while True:
        v_old = v.copy() # ä¿å­˜æ—§çš„çŠ¶æ€å€¼
        for s in range(nS):
            a = policy[s] # å½“å‰ç­–ç•¥ä¸‹sçŠ¶æ€é€‰çš„åŠ¨ä½œ ç¡®å®šæ€§ç­–ç•¥ï¼Ÿ
            v[s] = R[s, a] + gamma * np.dot(P[s, a], v_old)

        if np.max(np.abs(v - v_old)) < tol: # å¦‚æœçŠ¶æ€å€¼å˜åŒ–å°äºé˜ˆå€¼ï¼Œåˆ™åœæ­¢è¿­ä»£
            break
    return v # è¿”å›æœ€ç»ˆçš„çŠ¶æ€å€¼


# ----------- å·¥å…·å‡½æ•°ï¼šè®¡ç®—åŠ¨ä½œå€¼ q_pi(s,a) ----------------------
def compute_q_from_v(v):
    """
    æ ¹æ® v_pi(s) è®¡ç®— q_pi(s,a)

    q(s,a) = R(s,a) + gamma * sum_s' P(s,a,s') * v(s')
    """
    q = np.zeros((nS, nA))
    for s in range(nS): # éå†æ‰€æœ‰çŠ¶æ€
        for a in range(nA): # éå†æ‰€æœ‰åŠ¨ä½œ
            q[s,a] = R[s,a] + gamma * np.dot(P[s,a], v) # è®¡ç®—qå€¼ï¼šR(s,a) + gamma * sum_s' P(s,a,s') * v(s')
    return q # è¿”å›æœ€ç»ˆçš„qå€¼


# ----------- ç­–ç•¥æ”¹è¿›ï¼špolicy_improvement(v) -----------------------
def policy_improvement(v):
    """
    axis=0 â†’ åœ¨â€œç«–æ–¹å‘â€æ“ä½œï¼ˆè·¨è¡Œï¼‰
    axis=1 â†’ åœ¨â€œæ¨ªæ–¹å‘â€æ“ä½œï¼ˆè·¨åˆ—ï¼‰
    
    çŠ¶æ€ s	a=0	 a=1  a=2
    A(0)   ...	...  ...
    B(1)   ...	...  ...
    C(2)   ...	...  ...
    æ ¹æ® v(s) çš„è´ªå¿ƒç­–ç•¥æ”¹è¿›:
        Ï€'(s) = argmax_a q(s,a)
    è¿”å›æ–°çš„ç­–ç•¥
    np.argmax(q, axis=1) è¡¨ç¤ºï¼š

    å¯¹ æ¯ä¸€è¡Œ æ±‚æœ€å¤§å€¼çš„ç´¢å¼•ã€‚ ä¹Ÿå°±æ˜¯è¯´ï¼š
    å¯¹çŠ¶æ€ Aï¼Œæˆ‘ä»¬æ‰¾åˆ°å“ªä¸ªåŠ¨ä½œçš„ q å€¼æœ€å¤§
    å¯¹çŠ¶æ€ Bï¼Œä¹Ÿæ˜¯æ‰¾åˆ°å“ªä¸ªåŠ¨ä½œæœ€å¥½
    çŠ¶æ€ C åŒç†
    
    """
    q = compute_q_from_v(v)
    new_policy = np.argmax(q, axis=1)
    return new_policy, q


# ----------- ç­–ç•¥è¿­ä»£ä¸»å¾ªç¯ -------------------------------------
def policy_iteration():
    # åˆå§‹åŒ–ç­–ç•¥ï¼ˆéšä¾¿ç»™ï¼‰
    policy = np.zeros(nS, dtype=int) # åˆå§‹ç­–ç•¥ä¸º0ï¼Œè¡¨ç¤ºåœ¨æ¯ä¸ªçŠ¶æ€éƒ½é‡‡å–åŠ¨ä½œ0

    print("åˆå§‹ç­–ç•¥:", policy)

    for i in range(10):
        print(f"\n===== ç¬¬ {i+1} æ¬¡è¿­ä»£ =====")

        # 1) ç­–ç•¥è¯„ä¼°
        v = policy_evaluation(policy, gamma=gamma)
        print("v_pi(s):", v)

        # 2) ç­–ç•¥æ”¹è¿›
        new_policy, q = policy_improvement(v)
        print("q_pi(s,a):")
        print(q)
        print("æ”¹è¿›åçš„ç­–ç•¥:", new_policy)

        # è‹¥æ”¶æ•›ï¼šç­–ç•¥ä¸å†å˜åŒ–
        if np.all(new_policy == policy):
            print("\nğŸ‰ ç­–ç•¥æ”¶æ•›ï¼æœ€ç»ˆæœ€ä¼˜ç­–ç•¥ Ï€* =", new_policy)
            return new_policy, v, q

        policy = new_policy

    return policy, v, q


# ------------------- è¿è¡Œç­–ç•¥è¿­ä»£ ----------------------
optimal_policy, v_star, q_star = policy_iteration()

print("\n------------- æœ€ç»ˆç»“æœ -------------")
print("æœ€ä¼˜ç­–ç•¥ Ï€*(s):", optimal_policy)
print("æœ€ä¼˜çŠ¶æ€å€¼ v*(s):", v_star)
print("æœ€ä¼˜åŠ¨ä½œå€¼ q*(s,a):\n", q_star)

print("\næ ¹æ®ç†è®ºï¼šv*(s) = max_a q*(s,a):")
print(np.max(q_star, axis=1))
