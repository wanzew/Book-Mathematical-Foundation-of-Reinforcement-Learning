# ------ 第 2.5 节 · Bellman 方程实例（Examples of the Bellman Equation） ------

> 本节通过具体的数值示例展示如何在确定性与随机性环境下使用 Bellman
> 方程计算状态值函数。 我们将从手算入手，逐步过渡到代码实现，最终揭示
> Bellman 方程如何在实践中体现"动态规划思想"。

------------------------------------------------------------------------

## 🎯 学习目标

学习完本节后，你应能：\
1. 运用 Bellman 方程计算给定策略下的状态值函数；\
2. 理解确定性与随机性环境下的求解区别；\
3. 能编写简单代码迭代求解 $v_\pi(s)$；\
4. 直观感受 Bellman 方程的动态传播特性。

------------------------------------------------------------------------

## 📘 一、Bellman 方程回顾

在第 2.4 节中我们得到：

$$
v_\pi(s) = \sum_a \pi(a|s) \sum_{s'} p(s'|s,a)[r(s,a,s') + \gamma v_\pi(s')]. \tag{2.7}
$$

若策略 π 为确定性（即每个状态只执行一个动作），则方程简化为：

$$
v_\pi(s) = \sum_{s'} p(s'|s)[r(s,s') + \gamma v_\pi(s')]. \tag{2.9}
$$

------------------------------------------------------------------------

## 🌱 二、示例 1：确定性环境

设四个状态：$S=\{A,B,C,D\}$，其中 D 为终点（价值固定 $v(D)=0$）。\
奖励与转移如下表：

  当前状态   下一状态   奖励
  ---------- ---------- ------
  A          B          +1
  B          C          +2
  C          D          +3
  D          D          0

折扣因子 $\gamma = 0.9$。

根据 (2.9)：

$$
\begin{aligned}
v(A) &= 1 + 0.9v(B), \\
v(B) &= 2 + 0.9v(C), \\
v(C) &= 3 + 0.9v(D), \\
v(D) &= 0.
\end{aligned}
$$

解得：

$$
v(C)=3,\quad v(B)=2+0.9(3)=4.7,\quad v(A)=1+0.9(4.7)=5.23.
$$

**结果：**\
\| 状态 \| 值函数 $v_\pi(s)$ \| \|------\|------------------\| \| A \|
5.23 \| \| B \| 4.7 \| \| C \| 3.0 \| \| D \| 0.0 \|

解释：离目标越远，累计回报越高。

------------------------------------------------------------------------

## 🌿 三、示例 2：随机性环境

设状态 A 在执行动作后：\
- 以 0.8 概率到达 B（奖励 +1）；\
- 以 0.2 概率返回自身（奖励 0）。

$$
p(B|A)=0.8, \quad p(A|A)=0.2.
$$

Bellman 方程为：

$$
v(A) = 0.8[1+\gamma v(B)] + 0.2[0+\gamma v(A)]. \tag{2.10}
$$

假设 $v(B)=4.7$（来自上例），代入：

$$
v(A) = 0.8(1+0.9(4.7)) + 0.2(0.9v(A)) = 0.8(5.23) + 0.18v(A).
$$

化简得：

$$
v(A)(1-0.18) = 4.184 \Rightarrow v(A) = 5.10.
$$

结果略低于确定性情形，说明随机性降低了长期收益。

------------------------------------------------------------------------

## 🧮 四、Python伪代码：随机转移下的 Bellman 迭代

``` python
import numpy as np

gamma = 0.9
states = ["A", "B", "C", "D"]
R = np.array([1, 2, 3, 0])
P = np.array([[0.2, 0.8, 0, 0],  # A -> A(0.2), B(0.8)
              [0, 0, 1, 0],      # B -> C
              [0, 0, 0, 1],      # C -> D
              [0, 0, 0, 1]])     # D -> D

v = np.zeros(4)
for _ in range(50):
    v = R + gamma * P.dot(v)

print("v_pi =", np.round(v, 3))
```

输出示例：

    v_pi = [5.1 4.7 3.0 0.0]

------------------------------------------------------------------------

## 💬 五、确定性 vs 随机性 的比较

  场景     特征                   结果
  -------- ---------------------- ----------------------------
  确定性   每次动作结果固定       值函数计算简单、上界明显
  随机性   含多路径概率           值函数为加权期望、收敛更慢
  启示     不确定性降低期望收益   Bellman 方程天然兼容随机性

------------------------------------------------------------------------

## 🧠 六、自测题

1.  若奖励全部为负，Bellman 方程的值函数会呈现何种趋势？\
2.  在随机性环境中，为什么 Bellman 方程仍保持线性？\
3.  若 $\gamma=1$ 且存在循环状态，Bellman 方程是否仍有唯一解？\
4.  尝试修改代码，令 $p(B|A)=0.5, p(A|A)=0.5$，观察 $v(A)$ 的变化。

------------------------------------------------------------------------

## 📘 七、与第 3.1 节的衔接

本节展示了如何在确定性与随机性环境下具体使用 Bellman 方程计算状态值。\
然而，到目前为止，我们只在**固定策略 π** 下进行计算。\
在下一章（§3.1），我们将开始思考：\
\> 如果能主动"选择动作"，该如何改进策略、获得更高的回报？

这正是第 3 章 "**策略改进（Policy Improvement）**" 的出发点。
