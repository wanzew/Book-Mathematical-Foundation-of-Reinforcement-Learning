
import numpy as np

gamma = 0.9
# R 是每个状态下的即时奖励（reward），R[0]=1 表示从状态0(A)出发获得的奖励为1，R[1]=2 表示状态1(B)奖励2，R[2]=0 为状态2(C)奖励0
R = np.array([1, 2, 0])
# 转移概率矩阵P，每个元素 P[s, s'] 表示：从状态s出发，按固定策略的动作后，下一步转移到状态s'的概率
# 例如 P[0,1]=1 表示状态0（A）必定转移到状态1（B）
P = np.array([  # P[s, s']
    [0, 1, 0],  # 从状态0(A)：到A概率0，到B概率1，到C概率0
    [0, 0, 1],  # 从状态1(B)：到A概率0，到B概率0，到C概率1
    [0, 0, 1]   # 从状态2(C)：到A概率0，到B概率0，仍留在C的概率1
])

v = np.ones(3) # 不管初始值是多少，经过足够多次迭代后，状态值函数都会收敛到[2.8 2.  0. ]

for i in range(20):  # Bellman 迭代
    print("\tv_p", i,  " =", v)
    v = R + gamma * np.matmul(P, v)

print("状态值函数 v_pi =", v)

# 状态值函数 v_pi = [2.8 2.  0. ]