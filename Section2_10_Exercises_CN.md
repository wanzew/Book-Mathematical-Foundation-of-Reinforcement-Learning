# 📘 2.10 章节练习与思考（Exercises and Reflections）

> 本节提供针对第 2 章的关键练习题，帮助巩固对回报、状态值、Bellman
> 方程及其应用的理解。

------------------------------------------------------------------------

## 🧩 一、基础题（Basic）

**1. 定义题**\
写出以下各项的定义与意义：\
a) 回报 $G_t$\
b) 状态值函数 $v_\pi(s)$\
c) 动作值函数 $q_\pi(s,a)$\
d) Bellman 方程

**2. 推导题**\
从 $G_t = R_{t+1} + \gamma G_{t+1}$ 推导出 Bellman 方程的形式。

**3. 数值计算**\
若 $r=1$, $\gamma=0.9$, 且 $v(s')=5$，求：\
$$v(s)=r+\gamma v(s')$$

------------------------------------------------------------------------

## 💡 二、进阶题（Advanced）

**4. 理论题**\
证明：若 $v_1(s) ≥ v_2(s)$ 对所有 s 成立，则相应的策略 π₁ 至少不劣于
π₂。

**5. 概念题**\
解释为什么 Bootstrapping 能提高学习效率。

**6. 推广题**\
当环境具有非确定性转移（随机性），Bellman 方程中应如何修改？

**7. 应用题**\
假设环境有 3 个状态：$s_1,s_2,s_3$。\
奖励向量 $r=[0,1,1]$, 折扣因子 $\gamma=0.9$，转移矩阵：\
$$
P = 
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 1
\end{bmatrix}
$$

请计算：\
1) $v=(I-\gamma P)^{-1}r$\
2) 并解释每个状态值的含义。

------------------------------------------------------------------------

## 🔍 三、思考与反思（Reflection）

**8. 思考题**\
在强化学习中，我们为什么不直接最大化即时奖励？

**9. 对比题**\
比较蒙特卡洛方法与时序差分（TD）方法在计算 $v_\pi$ 时的差异。

**10. 开放问题**\
Bellman 方程是否可以扩展到部分可观测环境（POMDP）？若可以，应如何修正？

------------------------------------------------------------------------

## 💬 四、延伸阅读

-   Sutton & Barto, *Reinforcement Learning*, 2nd Edition, Chapter 3\
-   Bertsekas, *Dynamic Programming and Optimal Control*\
-   Silver et al., *Deterministic Policy Gradient Algorithms*

> 🔎 建议：尝试在 Python 中实现 "矩阵法求解 Bellman 方程"，并比较不同 γ
> 对收敛速度的影响。
