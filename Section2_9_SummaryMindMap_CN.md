# 📘 2.9 总结与思维导图（Summary and Concept Map）

> 本节对第 2
> 章的主要概念进行系统总结，帮助你从全局视角理解"状态值---动作值---Bellman
> 方程"之间的关系。

------------------------------------------------------------------------

## 🧭 一、章节核心脉络

  -------------------------------------------------------------------------------------------------------------------------------
  概念           数学形式                                                             含义                关键思想
  -------------- -------------------------------------------------------------------- ------------------- -----------------------
  **回报         $$G_t = R_{t+1} + \gamma R_{t+2} + \dots$$                           累计折扣奖励        衡量策略长期收益
  (Return)**                                                                                              

  **状态值函数   $$v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]$$                               状态下长期回报      策略表现的度量
  (vπ)**                                                                                                  

  **动作值函数   $$q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]$$                       状态-动作组合价值   控制问题关键指标
  (qπ)**                                                                                                  

  **Bellman      $$v_\pi(s)=\sum_a \pi(a|s)\sum_{s'}p(s'|s,a)[r+\gamma v_\pi(s')]$$   自洽递归关系        奠定值迭代基础
  方程**                                                                                                  
  -------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------

## 🧩 二、关系总图（文字形式）

            ┌───────────────┐
            │ 回报 G_t      │ ← 基于奖励与折扣的定义
            └──────┬────────┘
                   ↓
            ┌───────────────┐
            │ 状态值 vπ(s)  │ ← 期望的 G_t
            └──────┬────────┘
                   ↓
            ┌───────────────┐
            │ Bellman 方程   │ ← vπ 的递推定义
            └──────┬────────┘
                   ↓
            ┌───────────────┐
            │ 动作值 qπ(s,a)│ ← 展开一步：包含动作 a
            └──────┬────────┘
                   ↓
            ┌───────────────┐
            │ 最优值函数 v*, q*│ ← 加入 max 操作
            └────────────────┘

------------------------------------------------------------------------

## 💡 三、核心逻辑链

1.  **从经验到期望**：$G_t$ → $v_\pi(s)$\
2.  **从期望到递推**：$v_\pi(s)$ → Bellman 方程\
3.  **从状态到动作**：$v_\pi(s)$ → $q_\pi(s,a)$\
4.  **从任意策略到最优策略**：$\pi$ → $\pi^*$（加入 max 操作）

------------------------------------------------------------------------

## 📘 四、总结要点

-   Bellman 方程是强化学习的基础等式：\
    $$v = r + \gamma P v$$\
-   状态值与动作值是一对互补概念；\
-   折扣因子 $\gamma$ 决定了未来收益的重要程度；\
-   通过最大化 $q_\pi$，可以导出最优策略。

------------------------------------------------------------------------

## 🧠 自测题

1.  为什么 Bellman 方程可以看作"自洽条件"？\
2.  如何从 $v_\pi(s)$ 导出 $q_\pi(s,a)$？\
3.  若 $\gamma=1$，系统是否仍稳定？\
4.  请用一句话总结：Bellman 方程对强化学习的意义。

------------------------------------------------------------------------

## 💬 五、思维拓展

-   若我们允许 $\pi$ 为随机策略，是否仍能定义最优策略？\
-   Bellman 方程在非马尔可夫过程下会如何变化？\
-   在连续空间中（如机器人控制），状态值函数如何近似？
