# ------ 第 3.1 节 · 动机示例：如何改进策略（Motivating Example: How to Improve Policies） ------

> 本节通过一个具体的网格世界（Grid World）示例，引出"策略改进（Policy
> Improvement）"的思想。
> 我们将看到：**如何判断一个策略比另一个更好，以及如何系统地改进策略。**

------------------------------------------------------------------------

## 🎯 学习目标

学习完本节后，你应能：\
1. 理解"策略改进"的核心逻辑；\
2. 通过对比不同策略的状态值函数，判断策略优劣；\
3. 掌握策略改进原则的数学推导；\
4. 为后续 Bellman 最优方程 (§3.2) 打下直觉基础。

------------------------------------------------------------------------

## 📘 一、问题背景：为何需要改进策略？

在第 2 章中，我们已经学会如何计算某个策略 π 的状态值函数：

$$
v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s].
$$

但这只能"评估"策略，而不能告诉我们：\
\> 是否存在比 π 更好的策略？\
\> 如何从 π 逐步改进得到最优策略？

这就是策略改进的动机。

------------------------------------------------------------------------

## 🧩 二、文字化图景（Figure 3.1）

考虑一个 4×4 的网格世界（Grid World）：

    S0  S1  S2  S3
    S4  S5  S6  S7
    S8  S9  S10 S11
    S12 S13 S14 S15

-   起点在左上角（S0），目标在右下角（S15）。\
-   每个动作（上、下、左、右）都有奖励 $r=-1$。\
-   当到达 S15 时，回合结束（终止状态）。\
-   折扣因子 $\gamma=0.9$。

**策略 π₁：** 每个状态随机选择动作（等概率）。\
**策略 π₂：** 每个状态总是朝向目标移动（贪心策略）。

若分别计算其状态值函数，可得到：

  状态   $v_{\pi_1}(s)$   $v_{\pi_2}(s)$
  ------ ---------------- ----------------
  S0     -22              -14
  S5     -20              -10
  S10    -10              -4
  S15    0                0

→ 很明显：$\forall s,\; v_{\pi_2}(s) > v_{\pi_1}(s)$，说明 π₂ 优于 π₁。

------------------------------------------------------------------------

## 📐 三、策略改进的逻辑

我们希望通过当前的 $v_\pi(s)$ 来指导新的策略 π′。\
若在某个状态 s 存在动作 a，使得：

$$
q_\pi(s,a) > v_\pi(s),
$$

则执行 a 会比当前策略 π 在 s 下更好。\
因此我们定义新的策略：

$$
\pi'(s) = \arg\max_a q_\pi(s,a). \tag{3.1}
$$

这就是 **策略改进原则（Policy Improvement Theorem）** 的核心思想。

> **直觉解释：**\
> 如果我们在某些状态采取了比原策略更优的动作，整体策略就不会更差。

------------------------------------------------------------------------

## 💡 四、数学推导：为什么策略改进可行？

对于任意 s，定义：

$$
v_{\pi'}(s) = \mathbb{E}_{\pi'}[G_t | S_t=s].
$$

若对所有状态满足：

$$
q_\pi(s, \pi'(s)) \ge v_\pi(s), \quad \forall s, \tag{3.2}
$$

则根据期望的单调性：

$$
v_{\pi'}(s) \ge v_\pi(s), \quad \forall s. \tag{3.3}
$$

这意味着 π′ 至少不比 π 差。\
若存在状态使严格大于号成立，则 π′ 比 π 更优。

> 📘 **定理意义：**\
> 若在所有状态下，新的策略 π′ 选择的动作使得 $q_\pi(s,a)$ 不低于当前
> $v_\pi(s)$，\
> 则 π′ 至少不劣于 π。

------------------------------------------------------------------------

## 🔢 五、数值示例（γ=0.9）

考虑 3 个状态：A, B, C。\
每个动作带来奖励与转移：

  当前状态   动作   下一状态   奖励
  ---------- ------ ---------- ------
  A          →      B          +1
  B          →      C          +2
  C          →      C          0

策略 π₁ 固定执行 "→"，策略 π₂ 偶尔原地不动。

计算：

$$
v_{\pi_1}(A)=1+0.9v_{\pi_1}(B),\quad v_{\pi_1}(B)=2+0.9v_{\pi_1}(C),\quad v_{\pi_1}(C)=0.
$$

→ $v_{\pi_1}(A)=2.8, v_{\pi_1}(B)=2, v_{\pi_1}(C)=0$。

若 π₂ 在 B 原地不动，则 $v_{\pi_2}(B)=0$，明显变差。\
因此 π₁ 是更优策略。

这正是策略改进的思想：\
\> 在保持其他动作不变的情况下，只要找到局部更优动作，就能提升整体策略。

------------------------------------------------------------------------

## 🧮 六、Python伪代码：策略改进循环

``` python
import numpy as np

states = ["A", "B", "C"]
gamma = 0.9

# 奖励矩阵与转移矩阵
R = np.array([[0, 1, 0],
              [0, 0, 2],
              [0, 0, 0]])
P = np.array([[[0,1,0],
               [0,0,1],
               [0,0,1]]] * 3)

v = np.zeros(3)       # 初始状态值
policy = np.zeros(3)  # 策略（每状态的动作索引）

for iteration in range(10):
    # 策略评估
    for _ in range(5):
        v = np.sum(R, axis=1) + gamma * P[0].dot(v)

    # 策略改进
    q = np.zeros((3,3))
    for s in range(3):
        for a in range(3):
            q[s,a] = R[s,a] + gamma * P[s,a].dot(v)
        policy[s] = np.argmax(q[s])

print("改进后的策略:", policy)
print("状态值:", v)
```

该循环展示了如何：\
1. 评估当前策略的值函数；\
2. 按 $q_\pi(s,a)$ 选择更优动作；\
3. 逐步改进策略 ------ 这正是策略迭代（Policy Iteration）的原型。

------------------------------------------------------------------------

## 🧠 七、自测题

1.  为什么 $q_\pi(s,a) > v_\pi(s)$ 意味着可以改进策略？\
2.  若两个策略在某些状态下表现相同，它们是否等价？\
3.  如何在不直接计算 $q_\pi$ 的情况下进行策略改进？\
4.  为什么策略改进通常会收敛到最优策略？

------------------------------------------------------------------------

## 📘 八、与第 3.2 节的衔接

本节通过实例展示了**如何从策略评估走向策略改进**。\
然而，我们仍需更正式地刻画最优策略满足的数学条件。\
下一节 §3.2 将推导出 **Bellman 最优方程（Bellman Optimality
Equation）**， 它给出最优策略与最优值函数之间的递推关系，
并成为所有强化学习最优算法的理论核心。
