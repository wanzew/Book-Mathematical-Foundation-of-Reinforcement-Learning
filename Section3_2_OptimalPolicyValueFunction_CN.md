# ------ 第 3.1 节 · 最优策略与最优值函数 ------

> 本节是强化学习"控制问题"的起点。前两章我们学习了如何在给定策略 π
> 下评估状态值 $v_\pi$，
> 现在我们关心的问题是：**哪一个策略才是最优的？**\
> 我们将定义最优策略、最优状态值函数 $v^*$ 和最优动作值函数
> $q^*$，并理解它们的关系。

------------------------------------------------------------------------

## 🎯 学习目标

学习完本节后，你应能：\
1. 理解最优策略的定义与存在性；\
2. 掌握 $v^*(s)$ 与 $q^*(s,a)$ 的关系；\
3. 理解为什么多个最优策略可以共享同一个最优值函数；\
4. 看懂策略改进的逻辑，为下一节 Bellman 最优方程铺垫。

------------------------------------------------------------------------

## 📐 一、最优策略的定义

我们希望找到一个策略 $\pi^*$，使其在所有状态下都能获得**最大期望回报**。

定义如下：

$$
v^*(s) = \max_{\pi} v_\pi(s), \quad \forall s \in \mathcal{S} \tag{3.1}
$$

对应的最优策略定义为：

$$
\pi^* \in \arg\max_{\pi} v_\pi(s), \quad \forall s. \tag{3.2}
$$

含义：\
- $v_\pi(s)$ 表示策略 π 在状态 s 下的期望长期回报；\
- $v^*(s)$ 是所有策略中可达到的最大值；\
- 若存在多个策略都能达到 $v^*(s)$，它们都称为最优策略。

------------------------------------------------------------------------

## 💡 二、最优动作值函数

为了刻画"在状态 s 采取某个动作 a"的好坏，我们定义最优动作值函数：

$$
q^*(s,a) = \max_{\pi} q_\pi(s,a). \tag{3.3}
$$

它表示从状态 s 执行动作 a
后，并沿最优策略继续执行时能获得的最大期望回报。

两者的关系为：

$$
v^*(s) = \max_{a} q^*(s,a). \tag{3.4}
$$

> 🔎 直观理解：$v^*(s)$ 是该状态的"最优潜力"，而 $q^*(s,a)$
> 是执行特定动作的"最优得分"。

------------------------------------------------------------------------

## 🧩 三、文字化图景：策略改进示意（Figure 3.1）

设有两个策略：\
- π₁：倾向于向左移动（保守）\
- π₂：倾向于向右移动（冒进）

在网格世界中：\
- 终点 G 在右下角；\
- π₁ 经常在安全区徘徊，获得稳定但较低回报；\
- π₂ 会更积极前进，偶尔进入陷阱但总体更快到达 G。

结果：$v_{π₂}(s) > v_{π₁}(s)$，因此 π₂ 优于 π₁。\
我们称 π₂ 是 π₁ 的"改进策略（improved policy）"。

这正体现了策略改进定理的核心思想：\
\> 若 $q_{π₁}(s,π₂(s)) ≥ v_{π₁}(s)$ 对所有 s 成立，则 π₂ 至少不劣于 π₁。

------------------------------------------------------------------------

## 🔢 四、数值示例

考虑三状态 MDP：$S=\{A,B,C\}$，奖励如下：

  转移    奖励
  ------- ------
  A → B   +1
  B → C   +2
  C → C   0

折扣因子 $\gamma = 0.9$。

设策略 π₁ 始终按上表转移（确定性）。\
则：

$$
v(A) = 1 + 0.9v(B), \quad v(B) = 2 + 0.9v(C), \quad v(C)=0.
$$

求解得：\
$$
v(A)=1+0.9(2)=2.8, \quad v(B)=2, \quad v(C)=0.
$$

若另一个策略 π₂ 可直接从 A→C 奖励为 2，\
则：\
$$
v(A)=2, \quad v(B)=2, \quad v(C)=0.
$$

→ 可见 π₁ 优于 π₂，因为其在状态 A 的长期期望更高。

------------------------------------------------------------------------

## 🧮 五、Python伪代码：策略评估与改进框架

``` python
import numpy as np

# 状态与动作空间
states = ["A", "B", "C"]
gamma = 0.9

# 奖励矩阵与转移矩阵（简单示例）
R = np.array([[0, 1, 0],
              [0, 0, 2],
              [0, 0, 0]])
P = np.array([[[0,1,0],
               [0,0,1],
               [0,0,1]]] * 3)

# 策略评估：v_pi = r + gamma * P * v
v = np.zeros(3)
for _ in range(10):
    v = np.sum(R, axis=1) + gamma * P[0].dot(v)

# 策略改进：选择使 q(s,a) 最大的动作
q = np.zeros((3,3))
for s in range(3):
    for a in range(3):
        q[s,a] = R[s,a] + gamma * P[s,a].dot(v)

policy = np.argmax(q, axis=1)
print("改进后的策略:", policy)
```

该框架展示了"评估 → 改进"的核心循环，是策略迭代（Policy
Iteration）的基础。

------------------------------------------------------------------------

## 🧠 六、自测题

1.  若两个策略在所有状态下的 $v_\pi(s)$
    完全相等，它们是否都是最优策略？\
2.  解释为什么最优策略可以是多个，但 $v^*(s)$ 却唯一。\
3.  若 $q^*(s,a)=v^*(s)$ 对所有 a 成立，这说明什么？\
4.  举例说明在非确定性环境下，$q_\pi(s,a)$ 如何体现动作风险。

------------------------------------------------------------------------

## 📘 七、与第 3 章后续内容的衔接

本节定义了"最优"的概念与相关函数。\
在接下来的 §3.2--§3.3 中，我们将从这些定义出发，推导出 **Bellman
最优方程（Bellman Optimality Equation）**：\
它给出 $v^*(s)$ 与 $q^*(s,a)$
的递推关系，并揭示如何通过数值迭代获得最优策略。
