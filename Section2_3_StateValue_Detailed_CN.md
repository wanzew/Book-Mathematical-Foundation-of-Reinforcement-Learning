# ------ 第 2.3 节 · 状态值函数（State Value Function） ------

> 在前两节中，我们学习了回报（Return）及其递推形式。\
> 本节将进一步定义"状态值函数（State Value Function）"，
> 它是强化学习中最核心的数学对象，用于衡量策略在每个状态下的长期表现。

------------------------------------------------------------------------

## 🎯 学习目标

学习完本节后，你应能：\
1. 理解状态值函数 $v_\pi(s)$ 的定义与含义；\
2. 掌握其数学期望形式与计算逻辑；\
3. 理解状态值与策略之间的关系；\
4. 能通过数值与代码示例计算简单策略的 $v_\pi(s)$。

------------------------------------------------------------------------

## 📘 一、定义与公式

在给定策略 $\pi$ 下，从状态 $s$ 出发的期望累计折扣回报定义为：

$$
v_\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s] = \mathbb{E}_\pi \Big[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Big| S_t = s \Big]. \tag{2.4}
$$

其中：\
- $\mathbb{E}_\pi$ 表示"在策略 π 下的期望"；\
- $R_{t+k+1}$ 是第 $k$ 步后的奖励；\
- $\gamma$ 是折扣因子，$0 \le \gamma \le 1$。

> **直觉解释：**\
> $v_\pi(s)$ 表示在状态 s 下，如果之后一直按照策略 π
> 行动，未来能获得的平均收益。

------------------------------------------------------------------------

## 💡 二、与策略的关系

策略 π 决定了智能体在每个状态下采取何种动作，\
而状态值函数 $v_\pi(s)$ 则衡量这种选择的长期后果。

因此：\
- 若 π 改变，则 $v_\pi(s)$ 也随之改变；\
- 两个策略的优劣可通过比较其对应的 $v_\pi(s)$ 来判断。

> 一个好的策略应该让所有状态的 $v_\pi(s)$ 都尽可能大。

------------------------------------------------------------------------

## 🧩 三、从回报到状态值的展开

结合 (2.2) 的回报递推式：

$$
G_t = R_{t+1} + \gamma G_{t+1},
$$

两边取期望（在策略 π 下）：

$$
v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t=s]. \tag{2.5}
$$

这表明：\
\> **状态值函数本身也满足递推关系。**\
\> 当前状态的价值 = 即时奖励期望 + 折扣后的下一状态期望价值。

------------------------------------------------------------------------

## 🔢 四、数值示例

设有三状态系统 $S=\{A,B,C\}$：

  当前状态   下一状态   奖励   概率
  ---------- ---------- ------ ------
  A          B          +1     1.0
  B          C          +2     1.0
  C          C          0      1.0

折扣因子 $\gamma=0.9$。\
假设策略 π 总是按上表转移（确定性）。

递推方程组：

$$
\begin{aligned}
v_\pi(A) &= 1 + 0.9v_\pi(B), \\
v_\pi(B) &= 2 + 0.9v_\pi(C), \\
v_\pi(C) &= 0.
\end{aligned}
$$

求解得：\
$$
v_\pi(A)=2.8, \quad v_\pi(B)=2.0, \quad v_\pi(C)=0.0.
$$

解释：\
- 状态 A 离目标最远，因此需要两步才能获得高奖励，值最大；\
- C 为终点，价值为零。

------------------------------------------------------------------------

## 🧮 五、Python伪代码：策略评估（Policy Evaluation）

``` python
import numpy as np

gamma = 0.9
states = ["A", "B", "C"]
R = np.array([1, 2, 0])
P = np.array([[0, 1, 0],
              [0, 0, 1],
              [0, 0, 1]])

v = np.zeros(3)
for _ in range(20):  # 迭代更新
    v = R + gamma * P.dot(v)

print("v_pi =", v)
```

输出：

    v_pi = [2.8 2.0 0.0]

该循环实现了 (2.5) 的递推更新过程。

------------------------------------------------------------------------

## 💬 六、直观解释（图景化理解）

在网格世界中，$v_\pi(s)$ 可以看作**地形高度图（value landscape）**：\
- 越靠近目标，$v_\pi(s)$ 越高；\
- 障碍或陷阱区域的 $v_\pi(s)$ 较低；\
- 策略 π 相当于"沿着地形上升的方向"移动。

------------------------------------------------------------------------

## 🧠 七、自测题

1.  若两个策略的 $v_\pi(s)$ 在所有状态下完全相等，它们是否等价？\
2.  为什么 $v_\pi(s)$ 只取决于策略 π 与环境转移，而与时间 t 无关？\
3.  若环境有多个终止状态，$v_\pi(s)$ 如何定义？\
4.  对比 Bootstrapping 更新公式 (2.3) 与本节的期望形式
    (2.5)，它们的联系是什么？

------------------------------------------------------------------------

## 📘 八、与第 2.4 节的衔接

本节定义了状态值函数 $v_\pi(s)$，并说明了它的递推性质。\
接下来的 §2.4 将在此基础上推导出 **Bellman 方程（Bellman Equation）**，
它给出了 $v_\pi(s)$ 与未来状态值的精确数学关系，
为后续的动态规划与最优控制方法奠定基础。
