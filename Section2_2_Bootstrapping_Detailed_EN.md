
# —— 2.2节 · 自举与递归回报 ——

> 本节介绍了强化学习中的核心思想之一——**自举（Bootstrapping）**。  
> 自举使智能体能够在回合尚未结束时就更新对未来回报的估计，  
> 这构成了**动态规划（DP）**和**时序差分学习（TD）**的数学基础。

---

## 🎯 学习目标

完成本节后，你应该能够：  
1. 理解回报 $G_t$ 的递归定义；  
2. 阐述自举思想的逻辑与动机；  
3. 通过数值例子计算 $v(S_t)$ 的更新过程；  
4. 区分自举方法和蒙特卡罗方法的不同。

---

## 📘 1. 回报的定义

在强化学习中，**回报（Return）**表示从时刻 $t$ 开始，未来所有奖励的贴现和：

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \tag{2.1}
$$

如果回合在时刻 $T$ 终止，则有：

$$
G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}.
$$

---

## 📐 2. 回报的递归关系

式（2.1）可以重写为递归形式：

$$
\begin{aligned}
G_t &= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \\
    &= R_{t+1} + \gamma G_{t+1}.
\end{aligned} \tag{2.2}
$$

这被称作**回报的递归定义**。  
意思是：

> 当前回报 $G_t$ 等于即时奖励加上贴现后的未来回报。

---

## 💡 3. 自举（Bootstrapping）思想

在实际中，我们通常**无法准确知道 $G_{t+1}$**，  
因此用当前对下一个状态的估计 $v(S_{t+1})$ 进行替代：

$$
v(S_t) \leftarrow R_{t+1} + \gamma v(S_{t+1}). \tag{2.3}
$$

这就是**自举更新**：  
> 用我们自己的预测（$v(S_{t+1})$）来改进我们的估计（$v(S_t)$）。

通俗来说，智能体“踩在自己的肩膀上”不断优化预测——  
通过状态空间将未来知识逐步向后传播。

---

## 🧩 4. 直观例子

想象如下的五状态链式环境：

```
S0 → S1 → S2 → S3 → Goal
```

- 每前进一步奖励 $R=-1$；  
- 到达终点奖励 $R=0$；  
- 折扣因子 $\gamma=0.9$。

从终点向前递推：

$$
v(S_4)=0, \quad v(S_3)=-1+0.9(0)=-1, \\
v(S_2)=-1+0.9(-1)=-1.9, \quad v(S_1)=-2.71.
$$

→ 离目标越远，状态价值越低。  
这种向后递推的过程正是自举更新的工作方式。

---

## 🔢 5. 数值例子

已知：  
- $R_{t+1}=0.8$，  
- $\gamma=0.9$，  
- 当前估计 $v(S_{t+1})=1.0$。

则：

$$
v(S_t) = R_{t+1} + \gamma v(S_{t+1}) = 0.8 + 0.9(1.0) = 1.7.
$$

当 $v(S_{t+1})$ 提升到 1.2 时：

$$
v(S_t) = 0.8 + 0.9(1.2) = 1.88.
$$

智能体在学习过程中不断修正和改进自己的预测。

---

## 🧮 6. Python 实现：自举更新循环

```python
gamma = 0.9
alpha = 0.1  # 学习率

# 初始化状态价值
v = {"S0": 0.0, "S1": 0.0, "S2": 0.0, "S3": 0.0, "Goal": 0.0}
rewards = {"S0": -1, "S1": -1, "S2": -1, "S3": 0}

for episode in range(10):
    for s in ["S0", "S1", "S2", "S3"]:
        next_s = {"S0": "S1", "S1": "S2", "S2": "S3", "S3": "Goal"}[s]
        v[s] += alpha * (rewards[s] + gamma * v[next_s] - v[s])

print(v)
```

上述代码实现了**TD(0)** 学习，  
每次更新用*下一个状态的预测*，而不必等待完整回报的到来。

---

## 💬 7. 重点总结

| 概念     | 公式                                            | 直观解释                   |
| -------- | ----------------------------------------------- | -------------------------- |
| 回报     | $G_t = R_{t+1} + \gamma G_{t+1}$                | 递归累计奖励               |
| 自举     | $v(S_t) \leftarrow R_{t+1} + \gamma v(S_{t+1})$ | 用自身预测未来来更新当前值 |
| 蒙特卡罗 | $v(S_t) = \mathbb{E}[G_t]$                      | 需完整历程结局             |

自举为学习带来了高效与持续的更新，  
而蒙特卡罗方法则可获得无偏估计，但方差较大。

---

## 🧠 8. 自测题

1. 证明（2.2）能展开为（2.1）；  
2. 当 $\gamma=0$ 时，自举等价于什么？  
3. 为什么自举对于持续性（非回合式）任务是不可缺的？  
4. 比较自举与蒙特卡罗方法——它们有什么优劣？

---

## 📘 9. 与第2.3节内容衔接

本节展示了如何通过**递归回报估计**，实现不必等待回合结束的学习。  
在下一节（§2.3）中，我们将正式定义**状态价值函数** $v_\pi(s)$，  
它将自举思想推广为期望形式，并给出**贝尔曼方程**的基础。 
