# ------ 第 2.1 节 · 为什么回报重要（Why Are Returns Important?） ------

> 本节通过直观的例子引导我们理解强化学习的根本目标：\
> **不仅仅是获得高的即时奖励，而是最大化整个任务过程中的累计折扣回报。**

------------------------------------------------------------------------

## 🎯 学习目标

学习完本节后，你应能：\
1. 理解"回报（Return）"的定义与重要性；\
2. 明白为何强化学习关注的是长期收益，而非短期奖励；\
3. 能比较不同策略的长期价值差异；\
4. 掌握计算折扣回报的基本方法。

------------------------------------------------------------------------

## 📘 一、从奖励到回报

在强化学习中，智能体每步获得一个即时奖励
$R_{t+1}$，但单个奖励无法反映长期表现。\
因此我们引入 **折扣回报（Discounted Return）**：

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \tag{2.1}
$$

其中：\
- $R_{t+k}$：第 $k$ 步奖励；\
- $\gamma \in [0,1]$：**折扣因子**，控制未来奖励的重要性。

------------------------------------------------------------------------

## 💡 二、为什么要折扣？

1.  **防止无穷大**：当任务无限长时，折扣确保回报有限；\
2.  **建模时间偏好**：未来奖励往往不如当前奖励确定；\
3.  **提高稳定性**：折扣有助于收敛与学习稳定。

例如，若 $\gamma=1$，智能体完全"远见"；\
若 $\gamma=0$，则完全"短视"，只看当前奖励。

------------------------------------------------------------------------

## 🧩 三、策略的长期目标

强化学习的目标是找到一个最优策略 $\pi^*$，使得：

$$
\pi^* = \arg\max_\pi \mathbb{E}_\pi[G_t]. \tag{2.2}
$$

这称为 **最大化期望回报（maximize expected return）**。\
这也是所有强化学习算法的基本出发点。

------------------------------------------------------------------------

## 🔢 四、数值示例：三种策略比较

设一个三步任务：\
- 状态序列：$S_0 \to S_1 \to S_2 \to S_3$（终点）；\
- 奖励序列如下表； - 折扣因子 $\gamma=0.9$。

  策略   奖励序列       回报 $G_0$
  ------ -------------- --------------------------------
  π₁     \[0, 0, +1\]   $0 + 0.9(0) + 0.9^2(1) = 0.81$
  π₂     \[-1, +1\]     $-1 + 0.9(1) = -0.1$
  π₃     \[0, +1\]      $0 + 0.9(1) = 0.9$

→ **结果：** π₃ 优于 π₁，π₁ 优于 π₂。

这说明：即使某步奖励较低，只要长期收益更高，该策略仍更优。

------------------------------------------------------------------------

## 🧮 五、Python伪代码：计算累计回报

``` python
def discounted_return(rewards, gamma=0.9):
    G = 0.0
    for r in reversed(rewards):   # 从最后一步反向累积
        G = r + gamma * G
    return G

# 示例
pi1 = [0, 0, 1]
pi2 = [-1, 1]
pi3 = [0, 1]

print(discounted_return(pi1))  # 0.81
print(discounted_return(pi2))  # -0.1
print(discounted_return(pi3))  # 0.9
```

这段代码实现了式 (2.1) 的数值计算形式。\
在实际算法中，这一过程会在每个时间步滚动进行。

------------------------------------------------------------------------

## 💬 六、直觉总结

-   回报是强化学习的核心评价指标；\
-   折扣体现了"短视与远见"的平衡；\
-   不同策略通过期望回报比较优劣。

> 一句话总结：\
> **强化学习不追求眼前的奖励，而追求长远的回报。**

------------------------------------------------------------------------

## 🧠 七、自测题

1.  当 $\gamma=1$ 时，策略 π₁、π₂、π₃ 的比较结果是否会改变？\
2.  为什么需要使用期望 $\mathbb{E}_\pi[G_t]$ 而非单次样本的 $G_t$？\
3.  若奖励序列为 $[1, -2, +4]$ 且 $\gamma=0.5$，计算 $G_0$。\
4.  折扣因子太小会造成什么问题？太大会带来什么风险？

------------------------------------------------------------------------

## 📘 八、与第 2.2 节的衔接

本节确立了强化学习的核心目标------最大化折扣回报。\
然而直接计算 $G_t$ 需要等到任务结束，效率低。\
在下一节 §2.2 中，我们将引入 **Bootstrapping（自举）思想**，
通过递推关系 $G_t = R_{t+1} + \gamma G_{t+1}$ 来高效地估计回报，
让学习在任务尚未结束时就能持续进行。
